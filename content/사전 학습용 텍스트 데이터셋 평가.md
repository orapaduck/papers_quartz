그람 슈미츠 직교화를 사용하여 text embedding에 대한 직교 기저를 구한다.
-> 이는 각 문장에서 독립적인 성분들을 추출해낸 것이므로 데이터셋이 가지는 의미, 문장 구조 등에 대한 정보를 가지고 있을 것이다.
직교화 방법도 여러가지로 가능한가

발표나 보고서에서 시각화 많이 하자.
1. D4에서 clustering 및 dedup 같은거

사전학습의 중요성.
1. downstream task에 대한 성능 향상 및 일반 도메인 지식 증가 
2. LLM을 sLM으로 경량화하는 방법들  (knownledge distillation, quatinization, pruning, PEFT)

리소스 부족으로 인해 sLM을 학습시키는 것에 대한 당위성

기존 방법
1. dedup
2. pruning
3. quality
4. clustering
5. perplexity -> LLM의 학습을 통해 데이터를 평가...

평가 기준
1. 직교 벡터의 분산, 표준편차, 평균 등
2. 크기 분포
3. 직교화가 진행될수록 벡터의 크기가 점점 줄어들 것임 -> 변화 추이?
4. 직교화 과정에서 시간 복잡도 $O(n^{2})$ 줄이기


https://ar5iv.labs.arxiv.org/html/2312.01700
https://github.com/ZigeW/data_management_LLM?tab=readme-ov-file#data-quality

직교 변환 관련
https://velog.io/@riverdeer/paper-review-Ultradense-Word-Embeddings-by-Orthogonal-Transformation