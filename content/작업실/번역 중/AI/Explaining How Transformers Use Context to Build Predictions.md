---
Created: 2024-02-15
Last Modified: 2024-02-22
tags:
  - NLP
DOI: "\rhttps://doi.org/10.48550/arXiv.2305.12535"
ì™„ë£Œ ì—¬ë¶€: false
---
```text-progress-bar
progress:: 4/9.5
fill:ğŸŸ©
transition:ğŸŸ¨
empty:â—»ï¸
prefix:[
suffix:]
length:10
```
## Abstract
&emsp;ìì—°ì–´ ìƒì„± ëª¨ë¸ì€ ì´ì „ì˜ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì‹ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì…ë ¥ ê¸°ì—¬ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ, ì•ì„  ë‹¨ì–´ê°€ ì–´ë–»ê²Œ ë ˆì´ì–´ë¥¼ ê±°ì³ ëª¨ë¸ì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ ì•„ì§ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” Transformerì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì— ëŒ€í•œ ìµœê·¼ì˜ ë°œì „ì„ í™œìš©í•˜ê³  ì–¸ì–´ ìƒì„± ëª¨ë¸ì„ ë¶„ì„í•˜ëŠ” ì ˆì°¨ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ëŒ€ì¡°ì ì¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ì˜ ì„¤ëª…ì´ ì–¸ì–´ í˜„ìƒì˜ ì¦ê±°^[ë¬¸ë²•ì ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ì— ë¬´ì—‡ì´ ì™€ì•¼ í•œë‹¤ê³  ì¶”ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì£¼ëœ ë‹¨ì„œë“¤ì„ ì–¸ì–´ í˜„ìƒì˜ ì¦ê±°ë¼ê³  í‘œí˜„í•˜ëŠ” ë“¯í•˜ë‹¤. ì˜ˆì‹œëŠ” Table 2 ì°¸ì¡°.]ì™€ ì–´ë–»ê²Œ ì •ë ¬ë˜ëŠ”ì§€ ë¹„êµí•˜ê³ , ìš°ë¦¬ì˜ ë°©ë²•ì´ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ë° ì„­ë™ ê¸°ë°˜ì˜ ê¸°ì¤€ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ë” ì˜ ì •ë ¬ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ, Transformer ë‚´ë¶€ì˜ MLPsì˜ ì—­í• ì„ ì¡°ì‚¬í•˜ê³ , ì´ë“¤ì´ ë¬¸ë²•ì ìœ¼ë¡œ í—ˆìš©ë˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” íŠ¹ì§•ì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ì˜ ë°©ë²•ì„ ì‹ ê²½ ê¸°ê³„ ë²ˆì—­ ëª¨ë¸ì— ì ìš©í•˜ì—¬, ì´ë“¤ì´ ì˜ˆì¸¡ì„ êµ¬ì¶•í•˜ëŠ” ë° ì¸ê°„ê³¼ ìœ ì‚¬í•œ source-target alignment^[ì¼ë°˜ì ìœ¼ë¡œ alignmentëŠ” ë‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ ê°„ì˜ ëŒ€ì‘ì„ ë‚˜íƒ€ë‚´ëŠ” ë“¯ í•¨. source vectorë¥¼ ì•Œë§ì€ target vectorì— ëŒ€ì‘ ì‹œí‚¨ë‹¤ëŠ” ì ì—ì„œ ì •ë ¬ì´ë¼ê³  í‘œí˜„í•˜ëŠ” ë“¯]ì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.
## 1 Introduction
&emsp;ì–¸ì–´ ëª¨ë¸ë“¤, íŠ¹íˆ Transformer ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ë“¤ (Brown et al., 2020; Zhang et al., 2022a)ì€ ìµœê·¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì— í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ ëª¨ë¸ë“¤ì´ ì–´ë–»ê²Œ ì¸ê°„ì˜ ì–¸ì–´ì™€ ìœ ì‚¬í•œ ì–¸ì–´ë¥¼ ìƒì„±í•˜ëŠ” ì§€ì— ëŒ€í•œ ì´í•´ì—ëŠ” ì—¬ì „íˆ ê°„ê·¹ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • ìƒí™©ì—ì„œ ëª¨ë¸ì˜ ì‹¤íŒ¨ ì›ì¸ì„ ê²°ì •í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì´ë¡œ ì¸í•´ halluinationì„ í¬í•¨í•˜ê±°ë‚˜ ìœ í•´í•œ ë‚´ìš©ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&emsp;NLP ëª¨ë¸ ì˜ˆì¸¡ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì•ì„  ì—°êµ¬ë“¤ ì¤‘ ëŒ€ë‹¤ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ì¶œë ¥ ì°¨ì›ì„ ê°€ì§€ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ë‚˜ ìì—°ì–´ ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤ (Atanasova et al., 2020; Bastings et al., 2022; Zaman and Belinkov, 2022). ì´ ì—°êµ¬ ë¶„ì•¼ì—ëŠ” attention mechanism ë¶„ì„ì— ì¤‘ì ì„ ë‘” ë§ì€ ì—°êµ¬(Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020)ì™€ ì…ë ¥ ê¸°ì—¬ë„ ì ìˆ˜ë¥¼ ì–»ê¸° ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ë°©ë²•(Li et al., 2016a; Sundararajan et al., 2017)ì„ ì ìš©í•˜ëŠ” ì—°êµ¬ë„ í¬í•¨ë©ë‹ˆë‹¤.
![[Table1.png|200]]
<font color="#ff0000"><em>Table 1: </em></font>
&emsp;ìµœê·¼ ë“¤ì–´, ì—¬ëŸ¬ ì—°êµ¬ë“¤ì€ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ Transformerì˜ í•´ì„ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤ (Vaswani et al., 2017). Elhage et al. (2021)ì€ Figure 1ì— ì„¤ëª…ëœ Transformerë¥¼ ë‹¤ì–‘í•œ ìš”ì†Œ(MLPs, attention heads...)ê°€ residual streamì˜ í•˜ìœ„ ê³µê°„ì„ ì½ê³  ì“°ëŠ” residual streamì˜ ê´€ì ^[residual connectionìœ¼ë¡œ ì—°ê²°ëœ stream, íë¦„ì— ì •ë³´ë¥¼ ì¶”ê°€í•´ ë‚˜ê°€ëŠ” ê²ƒì„ write into  the residual streamì´ë¼ê³  í‘œí˜„í•œ ê±° ê°™ë‹¤. ì¦‰, ì´ ë…¼ë¬¸ì—ì„œëŠ” attentionê³¼ MLP ë“±ì„ residual streamì˜ ì •ë³´ë¥¼ ì½ê³  ìˆ˜ì •í•´ ë‚˜ê°€ëŠ” ì—­í• ì´ë¼ê³  í•´ì„í•œë‹¤ (Figure 1 ì°¸ì¡°).]ì—ì„œ ì—°êµ¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ attention headsê°€ ë§¥ë½ì„ íƒìƒ‰í•˜ì—¬ ë™ì¼í•œ í† í°ì˜ ì´ì „ ë°˜ë³µì„ ì°¾ê³  ë‹¤ìŒ í† í°ì„ ë³µì‚¬í•˜ëŠ” induction heads (Olsson et al., 2022)ë‚˜ Indirect Object Identification (IOI) í•´ê²°ì— íŠ¹í™”ëœ heads(Wang et al., 2023) ê°™ì´ ì–¸ì–´ ëª¨ë¸ë“¤ì˜ íŠ¹ì • í–‰ë™ì„ ì„¤ëª…í•˜ëŠ” ë°ì— ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹„ìŠ·í•˜ê²Œ, Transformer ë‚´ë¶€ì˜ MLPs ë˜í•œ residual streamì— ì“°ëŠ” ìš”ì†Œë¡œ ì—°êµ¬ë˜ì–´ ì™”ìŠµë‹ˆë‹¤.  Geva et al. (2022)ì€ MLP ë¸”ë¡ì´ valueë¥¼ residualì— ì¶”ê°€í•˜ëŠ” key-value meory ì²˜ëŸ¼ ë™ì‘í•˜ì—¬ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ê°€ ì˜ˆì¸¡ë˜ë„ë¡ í•  ìˆ˜ ìˆìŒì„ ê´€ì¸¡í•˜ì˜€ìŠµë‹ˆë‹¤.

&emsp;ë” ë‚˜ì•„ê°€, attention heads, output weigh matrix ê·¸ë¦¬ê³  layer normalizationìœ¼ë¡œ êµ¬ì„±ëœ transformerì˜ attention mechnismì€ í•´ì„ ê°€ëŠ¥í•œ ì‘ì—…ìœ¼ë¡œ ë¶„í•´ ê°€ëŠ¥í•˜ê³  (Kobayashi et al., 2020, 2021), ì‹ ë¢°ì„±ì´ ë§¤ìš° ë†’ë‹¤ê³  ì¦ëª…ëœ ë ˆì´ì–´ë³„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Ferrando et al., 2022b,a).

&emsp;ì´ ì—°êµ¬ì—ì„œ ìš°ë¦¬ëŠ” Transformers language generatorsì˜ ì˜ˆì¸¡ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ attention ë¶„í•´ì™€ í•¨ê»˜ residual stream analysisì˜ ê´€ì ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ê° ë ˆì´ì–´ì—ì„œ ê°ê°ì˜ token representationì— ì˜í•´ ë”í•´ì§€ê±°ë‚˜ ë¹¼ì§„ logitì˜ ì–‘ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ì–´ë¥¼ ê±°ì³ ì§‘ê³„í•¨ìœ¼ë¡œì¨ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ logit ê¸°ì—¬ë„ë¥¼ ì¶”ì í•©ë‹ˆë‹¤ (*Logit* explanation). ì¶”ê°€ì ìœ¼ë¡œ, ALTI(Ferrando et al., 2022b)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ì •ë³´ì˜ í˜¼í•©ì„ ê³ ë ¤í•©ë‹ˆë‹¤ (*ALTI-Logit* explanation).

&emsp;ì œì•ˆëœ í•´ì„ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ë°©ì‹ì„ í‰ê°€í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ìµœê·¼ì— ì†Œê°œëœ constrastive explanation framework(Yin and Neubig, 2022)ë¥¼ ë”°ë¥´ë©° ì´ëŠ” ëª¨ë¸ì´ ì´ë¯¸ ëª‡ëª‡ ì–¸ì–´ì  phenomena evidenceì— ì˜í•´ ì„¤ëª…ëœ foil token ëŒ€ì‹  íŠ¹ì • tokenì„ ì˜ˆì¸¡í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ ìš°ë¦¬ëŠ” MLPsì˜ ì—­í• ì„ ë¶„ì„í•˜ê³  ê·¸ë“¤ì´ ë¬¸ë²•ì„ ë”°ë¥´ëŠ” predictionì„ ì„ íƒí•˜ëŠ” ë°ì— ë„ì›€ì„ ì¤€ë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” NMT ëª¨ë¸ë“¤ì´ ë²ˆì—­ë¬¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ëŒê³¼ ìœ ì‚¬í•œ source-target alignmentë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.[^1]

[^1]: ì´ ë…¼ë¬¸ì„ ë”°ë¥´ëŠ” ì½”ë“œëŠ” https://github.com/mt-upc/logit-explanations ì— ìˆìŠµë‹ˆë‹¤.
## 2 Approach
### 2.1 Residual Stream
![[images/Explaining How Transformers Use Context to Build Predictions/Figure1.png|200]]
*Figure 1: residual streamì— ì“°ëŠ” ëª¨ë“ˆë¡œì¨ í‘œí˜„ëœ Transformer ì–¸ì–´ ëª¨ë¸*

&emsp;ì–¸ì–´ ìƒì„±ì´ timestep $t$ë¥¼ ë”°ë¼ ì£¼ì–´ì§ˆ ë•Œ, ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì¶œë ¥[^2] $x_{t}^{L} \in \mathbb{R}$ì€ ë‹¤ìŒ token ì˜ˆì¸¡ì˜ logitì„ êµ¬í•˜ê¸° ìœ„í•´ unembedding matrix $U \in \mathbb{R}^{d \times |V|}$ë¥¼ ì‚¬ìš©í•˜ì—¬ token embedding spaceë¡œ ì‚¬ì˜ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ, ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì–»ê¸° ìœ„í•´ softmax í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤:
$$
P(x_{t}^{L})=softmax(x_{t}^{L}U)\tag{1}
$$
&emsp;Transformerì˜ residual connectionì€ ê° ë¸”ë¡ ì´í›„ì— ì—…ë°ì´íŠ¸ë˜ëŠ” ì •ë³´ì˜ streamìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (nostalgebraist, 2020; Elhage et al., 2021; Mickus et al., 2022). ë ˆì´ì–´ $l$ì—ì„œ $t$ ìœ„ì¹˜ì˜ residual streamì— "ì“°ëŠ”" MLPì™€ self-attention ë¸”ë¡ì„ $o_{t}^{l}$, $\tilde{x}_{t}^{l}$ì´ë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤ (Figure 1). residual streamì˜ ìµœì¢… ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
$$
x_{t}^{L}=\sum_{l}^{L}o_{t}^{l}+\sum_{l}^{L}\tilde{x}_{t}^{l}+x_{t}^0\tag{2}
$$
íŠ¹ì •í•œ next token predictionì˜ ìµœì¢… logit $w$ëŠ” residual streamì˜ ìµœì¢… ìƒíƒœì™€ $U$ì˜ $w$ë²ˆì§¸ ì—´[^3]ì„ ê³±í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤:
$$
\begin{align}
logit_{w}&=x_{t}^LU_{w} \\
&=(\sum_{l}^{L}o_{t}^{l}+\sum_{l}^{L}\tilde{x}_{t}^{l}+x_{t}^0)U_{w}
\end{align}\tag{3}
$$
ì„ í˜•ì„±ì— ì˜í•´:
$$
logit_{w}=\sum_{l}^{L}o_{t}^{l}U_{w}+\sum_{l}^{L}\tilde{x}_{t}^{l}U_{w}+x_{t}^{0}U_{w}\tag{4}
$$

![[images/Explaining How Transformers Use Context to Build Predictions/Figure2.png|400]]
*Figure 2: self attention ë¸”ë¡ì˜ ì¶œë ¥ì´ ê° ë ˆì´ì–´ì—ì„œ $w$ì˜ logitì„ ì—…ë°ì´íŠ¸í•œë‹¤ (ì™¼ìª½). logitì˜ ì—…ë°ì´íŠ¸ëŠ” ê° input tokenì— ëŒ€í•´ ë¶„í•´ë  ìˆ˜ ìˆë‹¤ (ì˜¤ë¥¸ìª½).*

[^2]: ìš°ë¦¬ëŠ” ì´ë¥¼ ì—´ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì„ ì„ í˜¸í•©ë‹ˆë‹¤.
[^3]: ìš°ë¦¬ê°€ í–‰ë ¬ $B$ì˜ jë²ˆì§¸ í–‰ì„ $B_{:, j}$ ëŒ€ì‹ ì— $B_{j}$ë¼ê³  í‘œê¸°í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•œë‹¤ëŠ” ì ì„ ì•Œì•„ë‘ì„¸ìš”.
### 2.2 Multi-head Attention as a Sum of Vectors
&emsp;Kobayashi et al. (2021)ì˜ Post-LN self attention ë¸”ë¡ ë¶„í•´ì— ì˜ê°ì„ ë°›ì•„, ìš°ë¦¬ëŠ” í˜„ì¬ì˜ LMsì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” Pre-LN ì„¤ì •ì— ìœ ì‚¬í•œ ì ‘ê·¼ ë°©ë²•ì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤ (ì „ì²´ ìœ ë„ ê³¼ì •ì€ ë¶€ë¡ A ì°¸ì¡°). ê° ìƒì„± ë‹¨ê³„ $t$ì—ì„œ self-attention ë¸”ë¡ì˜ ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
$$
\tilde{x}_{t}^{l}=\sum_{j}^{t}T_{t,j}^l(x_{j}^{l-1})+b_{O}^{l}\tag{5}
$$
$T_{i,j}^l:\mathbb{R}^{d}\mapsto \mathbb{R}^{d}$ì´ ê° ë ˆì´ì–´ì˜ input token representation (ë˜ëŠ” residual stream) $x_{j}^{l-1} \in \mathbb{R}^d$ì— ì ìš©ëœ Affine transformationì´ë¼ê³  í•˜ë©´:
$$
T_{i,j}^{l}(x_{j}^{l-1})=\sum_{h}^{H}(x_{j}^{l-1}L^{l}W_{V}^{l,h}A_{t,j}^{l,h}W_{O}^{l,h})\tag{6}
$$
 $W_{V}^{l,h} \in \mathbb{R}^{d \times d_{h}}$ëŠ” valueë¥¼ ì´ë£¨ëŠ” í–‰ë ¬, $W_{O}^{l,h} \in \mathbb{R}^{d_{h} \times d}$ attention ì¶œë ¥ í–‰ë ¬ (head ë‹¹), ê·¸ë¦¬ê³  ì´ì— ëŒ€ì‘ë˜ëŠ” biasëŠ” $b_{O}^{l} \in \mathbb{R}^d$ì´ë‹¤. ì´ë•Œ $A^{l,h} \in \mathbb{R}^{t \times t}$ëŠ” attention weight í–‰ë ¬, $\theta^{l, h} \in \mathbb{R}^{d}$ëŠ” biasì—ì„œ ìœ ë˜í•œ remaining terms ê·¸ë¦¬ê³  $L^l \in \mathbb{R^{d \times d}}$ëŠ” centering, normalizing ê·¸ë¦¬ê³  layer normalizationì˜ scaling ì—°ì‚°ì„ í†µí•©í•œ ê²ƒì´ë‹¤ (ë¶€ë¡ A ì°¸ì¡°).
### 2.3 Layer-wise Contributions to the Logits
ì‹ (4)ì™€ ì‹ (5)ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤[^4] :
$$
\begin{align}
logit_{w}&=\sum_{l}^{L}\underbrace{o_{t}^{l}U_{w}}_{\Delta logit_{w\leftarrow MLP^{l}}^{l}}+\sum_{l}^{L}\underbrace{\sum_{j}^{t}T_{t,j}^l(x_{j}^{l-1}U_{w})}_{\Delta logit_{w\leftarrow Self-attn^{l}}^{l}}+x_{t}^{0}U_{w}
\end{align}\tag{7}
$$
&emsp;ê° self-attentionì— ëŒ€í•œ logitì˜ ë³€í™”ëŸ‰ $\Delta logit_{w\leftarrow Self-attn^{l}}^{l}$ì€ ê°ê°ì˜ $x_{j}^{l-1}$â€‹ì— ëŒ€í•œ ê°œë³„ ì—…ë°ì´íŠ¸ë¡œ í™•ì¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê·¸ë¦¼ 2 ì°¸ì¡°). ê·¸ëŸ¬ë¯€ë¡œ, output token $w$ì— ëŒ€í•œ ê° ë ˆì´ì–´ì˜ input token representationì˜ ê¸°ì—¬ëŠ” $w$ì˜ logitì„ ë³€í™” ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤:
$$
\Delta logit_{w\leftarrow x_{j}^{l-1}}^l=T_{i,j}^{l}(x_{j}^{l-1})U_{w}\tag{8}
$$
ì´ì™€ ë¹„ìŠ·í•˜ê²Œ, logitì˜ ë³€í™”ëŠ” ì‹ (6)ì˜ affine transformationì— unembedding í–‰ë ¬ì„ ê³±í•˜ì—¬ head levelì—ì„œ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[^4]: biasëŠ” ê³µê°„ì„ ì ˆì•½í•˜ê¸° ìœ„í•´ í‘œê¸°í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
### 2.4 Tracking Logit Updates to the Input Tokens
&emsp;ê°ê°ì˜ residual streamì´ ë ˆì´ì–´ ì „ë°˜ì— ê±¸ì³ tokenì˜ identityë¥¼ ìœ ì§€í•œë‹¤ê³  ê°€ì •í•˜ë©´, input token $s$ì— ì˜í•´ ìƒì„±ëœ $w$ì— ëŒ€í•œ ì „ì²´ logit ë³€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.
$$
\Delta logit_{w\leftarrow s}=\sum_{l}^{L}\Delta logit_{w\leftarrow x_{j=s}^l}^l\tag{9}
$$
ì´ëŠ” ì „ì²´ ë ˆì´ì–´ì—ì„œ së²ˆì§¸ í† í°ì˜ intermediate representationì— ì˜í•œ logit ë³€í™”ì˜ í•© ì…ë‹ˆë‹¤. ì´ì œë¶€í„°, ìš°ë¦¬ëŠ” ì´ë¥¼ $Logit$ explanationì´ë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤.

&emsp;í•˜ì§€ë§Œ, ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ê°ê°ì˜ residual streamì€ í˜¼í•©ëœ input tokenë“¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, $\Delta logit_{w\leftarrow x_{j}^{l-1}}^l$ì€ ëª¨ë¸ì˜ input token s=jì— ì˜í•œ ê²ƒì´ë¼ê³  ì§ì ‘ì ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” residual streamì— ë¬¸ë§¥ ì •ë³´ê°€ í˜¼í•©ë˜ëŠ” ê²ƒì„ ì¸¡ì •í•˜ì—¬ ëª¨ë¸ì˜ inputì— ëŒ€í•œ logit ë³€í™”ë¥¼ ì¶”ì í•˜ëŠ” ê²ƒì„ ì œí•œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ALTI (Ferrando et al., 2022b)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ALTIì™€ rollout methodë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•(Abnar and Zuidema, 2020; Mohebbi et al., 2023)ì€ token representationì´ ì´ì „ ë ˆì´ì–´ì˜ representationì„ ì„ í˜• ê²°í•©í•˜ì—¬ í˜•ì„±ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì¦‰, $x_{i}^{l}=\sum_{j}c_{i,j}^{l}x_{j}^{l-1}$ì´ê³  ì´ë•Œ $\sum_{j}c_{i,j}^{l}=1$ì…ë‹ˆë‹¤. $c_{i,j}^{l}$ì€ $x_{i}^l$ì— ëŒ€í•œ $x_{j}^{l-1}$ì˜ ê¸°ì—¬ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë ˆì´ì–´ë³„ ê³„ìˆ˜ í–‰ë ¬ì„ ê³±í•¨ìœ¼ë¡œì¨ $M^{l}=C^{1} \cdot C^{2}\cdot\cdots C^{l}$ì„ ì–»ì„ ìˆ˜ ìˆê³  ì´ë¥¼ í†µí•´ ê° ì¤‘ ë ˆì´ì–´ì˜ representationì„ input tokenì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ $x_{i}^{l}=\sum_{s}m_{i,s}^{l}x_{s}^{0}$.

&emsp;$M^{l-1}$ì˜ ì—´ sëŠ” ë ˆì´ì–´ $l$ì— ì…ë ¥ë˜ëŠ” ê° token representationì— ì¸ì½”ë”©ëœ së²ˆì§¸ input tokenì˜ ê¸°ì—¬ë„ ë¹„ìœ¨ì„ í¬í•¨í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ í†µí•´ input token (Figure 3, ì˜¤ë¥¸)ì— ì˜í•œ next predicition token $w$ì˜ logit ë³€í™”ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
$$
\Delta logit_{w\leftarrow s}=\Delta logit_{w\leftarrow x^{l-1}}^{l}M_{s}^{l-1}\tag{10}
$$
&emsp;ë” ìì„¸í•œ ì„¤ëª…ì€ ë¶€ë¡ Bì— ìˆìŠµë‹ˆë‹¤. prediction token $w$ì— ëŒ€í•œ $s$ë²ˆì§¸ input tokenì˜ ìµœì¢…ì ì¸ ê¸°ì—¬ëŠ” ê° ë ˆì´ì–´ logit ë³€í™”ì˜ í•©ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
$$
\Delta logit_{w\leftarrow s}=\sum_{l}^{L}\Delta logit_{w\leftarrow s}^{l}\tag{11}
$$
ìš°ë¦¬ëŠ” ì´ ë°©ë²•ì„ $ALTI-Logit$ explanationì´ë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ë¬¸ë§¥ì ì¸ ì •ë³´ê°€ í˜¼í•©ë˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, $M^{l-1}$ì´ ë‹¨ìœ„ í–‰ë ¬ì´ ë˜ì–´ $Logit$ explanationì´ ëœë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ë‘ì„¸ìš” (ì‹ (9)).
### 2.5 Constrastive Explanations
&emsp;constrastive explanation (Yin and Neubig, 2022)ì€ ë˜ë‹¤ë¥¸ foil token $f$ ëŒ€ì‹ ì— ì™œ target token $w$ë¥¼ ì„ íƒí•˜ì˜€ëŠ”ì§€ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ê²°ì •ì„ ê° tokenì´ $w$ì™€ $f$ê°„ì˜ final logit difference($logit_{w-f}$)ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ í†µí•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹ (9)ì™€ ì‹(11)ì— ë”°ë¼, input tokenì˜ Constrastive Logitê³¼ Constrastive ALTI-Logit[^5] saliency scoreë¥¼ ê·¸ë“¤ì˜ logit ì°¨ì— ëŒ€í•œ ë³€í™”ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
$$
\Delta logit_{(w-f)\leftarrow s}=\Delta logit_{w\leftarrow s}-\Delta logit_{f\leftarrow s}\tag{12}
$$
[^5]: ì´ ë…¼ë¬¸ ì „ì²´ì—ì„œ ìš°ë¦¬ëŠ” Logitê³¼ ALTI-Logitì„ ëŒ€ì¡°ì  ë³€í™”ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
## 3 Experimental Setup
&emsp;ìš°ë¦¬ê°€ ì œì•ˆí•œ ë°©ë²•ì˜ ì„±ëŠ¥ì„ constrastive explanationì„ í†µí•´ í‰ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. Yin and Neubig (2022)ì— ë”°ë¼, ë¬¸ë²•ì ìœ¼ë¡œ ë§ëŠ” ì•½ê°„ ë³€í˜•ëœ ë¬¸ì¥ë“¤ì´ ì§ì§€ì–´ì§„ BLiMP dataset (Warstadt et al., 2020)ì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. 11ê°œì˜ subsetì€ 5ê°œì˜ ì–¸ì–´ì  í˜„ìƒì„ ë”°ë¦…ë‹ˆë‹¤: anaphor agreement, arguent structure, determiner-noun agreement, NPI licensing ê·¸ë¦¬ê³  subject-verb agreement.

|         Phenomena         |  ID   | Example(<font color="#00b050">Acceptable</font>/<font color="#ff0000">Unacceptable</font>)                   |
|:-------------------------:|:-----:| ------------------------------------------------------------------------------------------------------------ |
|     Anaphor Agreement     |  aga  | <u>Karla</u> could listen to <font color="#00b050">herself</font>/<font color="#ff0000">himself</font>.      |
|                           |  ana  | Eva approached <font color="#00b050">herself</font>/<font color="#ff0000">themselves</font>.                 |
|     Argument Stucture     |  asp  | Gerald is <u>hated</u> by the <font color="#00b050">teachers</font>/<font color="#ff0000">pie</font>.        |
| Determiner-Noun Agreement |  dna  | Eva has scared <u>these</u> <font color="#00b050">children</font>/<font color="#ff0000">child</font>.        |
|                           | dnai  | Tammy was observing <u>that</u> <font color="#00b050">man</font>/<font color="#ff0000">men</font>.           |
|                           | dnaa  | The driver sees <u>that</u> unlucky <font color="#00b050">person</font>/<font color="#ff0000">people</font>. |
|                           | dnaai | Phillip liked <u>that</u> smooth <font color="#00b050">horse</font>/<font color="#ff0000">horses</font>.     |
|       NPI Licensing       |  npi  | <u>Even</u> Danielle <font color="#00b050">also</font>/<font color="#ff0000">ever</font> leaves.             |
|  Subject-Verb Agreement   | darn  | The <u>grandfathers</u> of Diana <font color="#00b050">drink</font>/<font color="#ff0000">drinks</font>.     |
|                           | ipsv  | Many <u>people</u> <font color="#00b050">have</font>/has hidden <font color="#ff0000">away</font>.           |
|                           | rpsv  | Most <u>associations</u> <font color="#00b050">buy</font>/<font color="#ff0000">buys</font> those libraries. |
*Table 2: ì˜ˆì‹œ: Table 8ì˜ Yin and Neubig (2022)ì— ì˜í•´ ì‚¬ìš©ëœ BLiMP phenomenons (acceptable/unacceptableí•œ ë‹¨ì–´ë¥¼ boldë¡œ í‘œê¸°). ë°‘ì¤„ë¡œ í‘œì‹œëœ ë‹¨ì–´ë“¤ì€ ì–¸ì–´ì  í˜„ìƒì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ì¦ê±°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (ê·œì¹™ì— ë”°ë¼ ì¶”ì¶œë¨)*

&emsp;ê° ì–¸ì–´ì  í˜„ìƒì— ëŒ€í•´, ìš°ë¦¬ëŠ” spaCy (Honnibal and Montani, 2017)ì„ ì‚¬ìš©í•˜ì˜€ê³  (previous tokensì—ì„œ) ë¬¸ë²•ì  ìˆ˜ìš©ì„±ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±°ë¥¼ ì°¾ê¸° ìœ„í•´ Yin and Neubig (2022)ì˜ ê·œì¹™ì„ ë”°ëìŠµë‹ˆë‹¤ (Table 2). anaphor agreementë¥¼ ìœ„í•´, target tokenê³¼ ìƒí˜¸ ì—°ê´€ëœ ëª¨ë“  context tokenì„ ì–»ì—ˆìŠµë‹ˆë‹¤. Determiner-noun agreementì˜ ì¦ê±°ëŠ” ëŒ€ìƒì´ ë˜ëŠ” ëª…ì‚¬ì˜ determiner(í•œì •ì)ë¡œ ë¶€í„° ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. NPI licensingì—ì„œ, "even"ì´ë¼ëŠ” ë‹¨ì–´ëŠ” acceptableí•œ ëŒ€ìƒì—ì„œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆì§€ë§Œ, unacceptableí•œ ë‹¨ì–´ì—ì„œëŠ” ë‚˜íƒ€ë‚  ìˆ˜ ì—†ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, subject-verb agreement í˜„ìƒì—ì„œ, ë™ìƒì˜ í˜•íƒœëŠ” ëŒ€ìƒì´ ë˜ëŠ” ëª…ì‚¬ì™€ ìˆ˜ì ìœ¼ë¡œ ì¼ì¹˜í•´ì•¼ í•˜ë©°, ì´ëŠ” ì¦ê±°ë¡œì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Yin and Neubig (2022)ì™€ ë‹¬ë¦¬, ipsvì™€ rpsv subsetì— í¬í•¨ëœ ë¬¸ì¥ì˜ ëŒ€ë¶€ë¶„ì´ 'ì •ëŸ‰ì‚¬+ì£¼ì–´ì˜ ì¤‘ì‹¬ì–´+ë™ì‚¬'ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , ì •ëŸ‰ì‚¬ì™€ ì£¼ì–´ì˜ ì¤‘ì‹¬ì–´ ë‘˜ ë‹¤  agreementë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì œì™¸í–ˆìŠµë‹ˆë‹¤.

&emsp;ìš°ë¦¬ëŠ” ë¶„ì„ì— SVA (subject-verb agreement) (Linzen et al., 2016)ê³¼ Indirect Object Identification (IOI) (Wang et al. 2023, Fahamu, 2022) datasetì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. SVA datasetì—ëŠ” ì£¼ì–´ì™€ ë‹¤ë¥¸ ìˆ˜ì˜ ëª…ì‚¬ê°€ í¬í•¨ë˜ì–´ ìˆì–´ saliency methodë¥¼ í‰ê°€í•˜ëŠ” ë°ì— ì í•©í•©ë‹ˆë‹¤. Indirect object identification (IOI)ëŠ” 'After Lee and Evelyn went to the lake'ì™€ ê°™ì€ ì´ˆê¸° ì¢…ì†ì ˆì„ ê°€ì§„ ë¬¸ì¥ë“¤ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì§•ì´ë©°, ì´ì–´ì§€ëŠ” ì£¼ì ˆì€ 'Lee gave a grape to Evelyn'ê³¼ ê°™ìŠµë‹ˆë‹¤. ê°„ì ‘ ëª©ì ì–´ "Evelyn"ê³¼ ì£¼ì–´ "Lee"ëŠ” ì´ˆê¸° ì ˆì—ì„œ ë°œê²¬ë©ë‹ˆë‹¤. IOI datasetì˜ ëª¨ë“  ì˜ˆì‹œì—ì„œ, ì£¼ì ˆì€ ë‹¤ì‹œ ì£¼ì–´ë¥¼ ì°¸ì¡°í•˜ì—¬ ê°„ì ‘ ëª©ì ì–´ì— ê°ì²´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. IOI taskì˜ ëª©í‘œëŠ” ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ IOì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. IOIì˜ ì˜ˆì—ì„œ, IOë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê·œì¹™ì€ IO ìì‹ ì´ ì²« ì ˆì— ìˆì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

&emsp;ìš°ë¦¬ëŠ” HuggingFace library (Wolf et al., 2020)ë¥¼ í†µí•´ (Yin and Neubig, 2022) ì—ì„œì™€ ê°™ì´ GPT-2 XL (1.5B) model (Radford et al., 2019)ì„ ì‚¬ìš©í•˜ì˜€ê³ , GPT-2 Small (124M)ì™€ GPT-2 Large models (774M), OPT 125M (Zhang et al., 2022b) ê·¸ë¦¬ê³  BLOOMâ€™s 560M and 1.1B variants (Workshop et al., 2022)ê³¼ ê°™ì€ ë‹¤ë¥¸ autoregressive Transformer language models ë˜í•œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

**Alignment Metrics**&emsp;Yin and Neubig(2022)ì— ë”°ë¼, ìš°ë¦¬ëŠ” $evidence$ë¥¼ previous token ìˆ˜ ë§Œí¼ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” binary vector $b \in \mathbb{R}^{t}$ì´ê³ , evidenceì— í¬í•¨ë˜ëŠ” tokenì˜ ìœ„ì¹˜ë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì˜ˆì¸¡ì´ ì˜ì¡´í•˜ëŠ” tokenì€ ê·œì¹™ì— ì˜í•´ ì¶”ì¶œë©ë‹ˆë‹¤. explanationì€ ë§ˆì°¬ê°€ì§€ë¡œ $\in \mathbb{R}$ì¸ ë²¡í„°ì…ë‹ˆë‹¤. explanationê³¼ evidenceê°„ì˜ alignmentë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” MRR (Mean Reciprocal Analysis)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. tokenì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬, MRRì€ 


## References
