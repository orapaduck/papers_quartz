---
Created: 2024-01-10
Last Modified: 2024-01-20
tags:
  - NLP
DOI: "\rhttps://doi.org/10.48550/arXiv.1802.05365"
ì™„ë£Œ ì—¬ë¶€: false
---
```text-progress-bar
progress:: 3.5/8
fill:ğŸŸ©
transition:ğŸŸ¨
empty:â—»ï¸
prefix:[
suffix:]
length:10
```
## Abstract
&emsp;ë‹¨ì–´ì˜ ë³µì¡í•œ íŠ¹ì„±(e.g, ë¬¸ë²• ì˜ë¯¸)ì™€ ì´ë“¤ì´ ì–¸ì–´ì  ë§¥ë½ì— ë”°ë¼ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ” ì§€(i.e. ë‹¤ì˜ì–´)ë¥¼ ëª¨ë‘ ëª¨ë¸ë§í•˜ëŠ” ìƒˆë¡œìš´ ìœ í˜•ì˜ ê¹Šì€ ë¬¸ë§¥í™”ëœ ë‹¨ì–´ í‘œí˜„ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë‹¨ì–´ ë²¡í„°ëŠ” ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ë¥¼ í•™ìŠµì‹œí‚¨ bidirectional Language Model(ì´í•˜ biLM)ì˜ ë‚´ë¶€ ìƒíƒœì— ëŒ€í•´ í•™ìŠµëœ í•¨ìˆ˜ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í‘œí˜„ ë°©ì‹ì´ ë‹¤ë¥¸ ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì— ë„ì…ë  ìˆ˜ ìˆìœ¼ë©° question answering, textual entailment, sentiment analysisë¥¼ í¬í•¨í•˜ëŠ” 6ê°€ì§€ ê¹Œë‹¤ë¡œìš´ NLP ê³¼ì œì—ì„œ ìµœì‹  ê¸°ìˆ ë“¤ì„ í˜„ì €í•˜ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ ìš°ë¦¬ëŠ” ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ ë‚´ë¶€ ë ˆì´ì–´ë¥¼ ë…¸ì¶œì‹œí‚¤ëŠ” ê²ƒì´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¤€ê°ë… ì‹ í˜¸ë¥¼ í˜¼í•©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° ì¤‘ìš”í•˜ë‹¤ëŠ” ë¶„ì„ì„ ì œì‹œí•©ë‹ˆë‹¤.
## 1. Introduction
&emsp;ì‚¬ì „ í•™ìŠµëœ ë‹¨ì–´ í‘œí˜„ì€ ë§ì€ ìì—°ì–´ë¥¼ ì´í•´í•˜ëŠ” ëª¨ë¸ì—ì„œ í•µì‹¬ì ì¸ ìš”ì†Œì…ë‹ˆë‹¤. ì–´ì¨Œë“ , ê³ í’ˆì§ˆì˜ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì€ ì´ìƒì ìœ¼ë¡œ ë‹¨ì–´ì˜ ë³µì¡í•œ íŠ¹ì„±(e.g., ë¬¸ë²•ê³¼ ì˜ë¯¸)ê³¼ ì´ë“¤ì´ ì–¸ì–´ì  ë§¥ë½ì— ë”°ë¼ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€(i.e., ë‹¤ì˜ì–´ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒ)ë¥¼ ëª¨ë‘ ëª¨ë¸ë§í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ, ìš°ë¦¬ëŠ” ë‘ ë¬¸ì œë¥¼ ì§ì ‘ì ìœ¼ë¡œ í•´ê²°í•˜ê³ , ê¸°ì¡´ ëª¨ë¸ì— ì‰½ê²Œ í•©ì³ì§ˆ ìˆ˜ ìˆê³ , ì œì‹œëœ ëª¨ë“  ì–´ë ¤ìš´ ì–¸ì–´ ì´í•´ ë¬¸ì œë“¤ì—ì„œ ìµœì‹  ê¸°ìˆ ì˜ ìƒë‹¹í•œ ê°œì„ ì„ ì´ë£¬ ìƒˆë¡œìš´ í˜•íƒœì˜ deepÂ  contextualized word representationì„ ì†Œê°œí•©ë‹ˆë‹¤.

&emsp;ìš°ë¦¬ì˜ representationì€ ê° í† í°ì— ì „ì²´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ í•¨ìˆ˜ í‘œí˜„ì´ í• ë‹¹ëœë‹¤ëŠ” ì ì—ì„œ ê¸°ì¡´ ë‹¨ì–´ ìœ í˜• ì„ë² ë”©ê³¼ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. <font color="#ff0000">ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ë¥¼ ëª©ì ìœ¼ë¡œ í•™ìŠµëœ ì§ì§€ì–´ì§„ ì–¸ì–´ ëª¨ë¸ê³¼ í•¨ê»˜ í•™ìŠµëœ ì–‘ë°©í–¥ LSTMìœ¼ë¡œ ë¶€í„° ì–»ì€ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤</font>. ì´ëŸ¬í•œ ì´ìœ ë¡œ, ìš°ë¦¬ëŠ” ê·¸ë“¤ì„ ELMo(Embeddings from Language Models) representationsë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ë¬¸ë§¥í™”ëœ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì´ì „ì˜ ë°©ë²•ë“¤ê³¼ ë‹¬ë¦¬, ELMo representationsëŠ” ì–‘ë°©í–¥ LSTMì˜ ëª¨ë“  ë‚´ë¶€ ë ˆì´ì–´ë“¤ì˜ í•¨ìˆ˜ë¼ëŠ” ì ì—ì„œ ê¹ŠìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê° ì…ë ¥ ë‹¨ì–´ ìœ„ì— ìŒ“ì¸ ë²¡í„°ì˜ ì„ í˜• ì¡°í•©ì„ í•™ìŠµì‹œì¼°ê³ , ì´ëŠ” LSTMì˜ ìµœìƒìœ„ ë ˆì´ì–´ë§Œì„ ì‚¬ìš©í•  ë•Œ ë³´ë‹¤ ì„±ëŠ¥ì„ í˜„ì €í•˜ê²Œ ê°œì„ ì‹œì¼°ìŠµë‹ˆë‹¤.

Â &emsp;ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ ë‚´ë¶€ ìƒíƒœë“¤ì„ í•©ì¹˜ëŠ” ê²ƒì€ ë§¤ìš° í’ë¶€í•œ ë‹¨ì–´ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤. ê³ ìœ í•œ í‰ê°€ë¥¼ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ìƒìœ„ LSTMì˜ ìƒíƒœê°€ ë¬¸ë§¥ì˜ ì˜ì¡´ì„± ì¸¡ë©´ì„ í¬ì°©í•˜ëŠ” ë°˜ë©´ í•˜ìœ„ LSTMì˜ ìƒíƒœëŠ” ë¬¸ë²•ì  ì¸¡ë©´ì„ í¬ì°©í•˜ëŠ” ê²ƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë™ì‹œì— ëª¨ë“  ì´ëŸ¬í•œ ì‹ í˜¸ë“¤ì„ ë…¸ì¶œì‹œí‚¤ëŠ” ê²ƒì€ í•™ìŠµëœ ëª¨ë¸ì´ ê° ìµœì¢… ì‘ì—…ì— ê°€ì¥ ìœ ìš©í•œ ì¤€ê°ë… ìœ í˜•ì„ ì„ íƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë§¤ìš° ìœ ìµí•©ë‹ˆë‹¤.

Â &emsp;ê´‘ë²”ìœ„í•œ ì‹¤í—˜ë“¤ì„ í†µí•´ ELMo representationì´ ì‹¤ì œë¡œ ë§¤ìš° ì˜ ì‘ë™í•œë‹¤ëŠ” ì‚¬ì‹¤ì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € ê·¸ë“¤ì´ question answering, textual entailment, sentiment analysisë¥¼ í¬í•¨í•˜ëŠ” 6ê°œì˜ ë‹¤ì–‘í•˜ê³  ì–´ë ¤ìš´ ì–¸ì–´ ì´í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ì¡´ì˜ ëª¨ë¸ì— ì‰½ê²Œ ì¶”ê°€ë  ìˆ˜ ìˆìŒì„ ë³´ì…ë‹ˆë‹¤. ELMo representationì„ ì¶”ê°€í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ ëª¨ë“  ì¼€ì´ìŠ¤ì—ì„œ ìµœì‹  ê¸°ìˆ ë“¤ì„ ìƒë‹¹íˆ ê°œì„ ì‹œì¼°ìœ¼ë©° ìƒëŒ€ ì˜¤ë¥˜ë¥¼ 20% ì´ìƒ ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤. ì§ì ‘ì ì¸ ë¹„êµê°€ ê°€ëŠ¥í•œ ê³¼ì œì—ì„œ, ELMoëŠ” ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ ê³„ì‚°í•˜ëŠ” ë¬¸ë§¥í™”ëœ representationì¸ CoVeë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ELMoì™€ CoVeì— ëŒ€í•œ ë¶„ì„ì€ deep representationì´ LSTMì˜ ìµœìƒìœ„ ë ˆì´ì–´ë¡œ ë¶€í„° ì–»ì€ representation ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í•™ìŠµëœ ëª¨ë¸ê³¼ ì½”ë“œëŠ” ê³µê°œë˜ì—ˆìœ¼ë©°, ìš°ë¦¬ëŠ” ELMoê°€ ë§ì€ ë‹¤ë¥¸ NLP ë¬¸ì œë“¤ì— ëŒ€í•´ ìœ ì‚¬í•œ ì´ì ì„ ì œê³µí•  ê²ƒì´ë¼ê³  ì˜ˆìƒí•©ë‹ˆë‹¤.
## 2. Related work
&emsp;ë¼ë²¨ë§ ë˜ì§€ ì•Šì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ë²•ì , ì˜ë¯¸ì  ì •ë³´ë¥¼ ì¶”ì¶œí•´ë‚´ëŠ” ëŠ¥ë ¥ ë•ë¶„ì—, ì‚¬ì „ í•™ìŠµëœ ë‹¨ì–´ ë²¡í„°ëŠ” question answering, textual entailment, semantic role labeling ë“±ì„ í¬í•¨í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ìµœì‹  NLP ì•„í‚¤í…ì²˜ì—ì„œ ì¼ë°˜ì ì¸ êµ¬ì„± ìš”ì†Œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ì–´ì¨Œë“ , ë‹¨ì–´ ë²¡í„° í•™ìŠµì„ ìœ„í•œ ì´ëŸ¬í•œ ì ‘ê·¼ë“¤ì€ ê° ë‹¨ì–´ë§ˆë‹¤ ë¬¸ë§¥ì— ë¹„ì˜ì¡´ì ì¸ í•˜ë‚˜ì˜ representationë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

&emsp;ì´ì „ì— ì œì‹œëœ ë°©ë²•ë“¤ì€ í•˜ìœ„ ë‹¨ì–´ ì •ë³´ë¥¼ í’ë¶€í•˜ê²Œ í•˜ê±°ë‚˜ ê° ë‹¨ì–´ì˜ ì˜ë¯¸ì— ëŒ€í•´ ë³„ë„ì˜ ë²¡í„°ë¥¼ í•™ìŠµí•˜ì—¬ ê¸°ì¡´ ë‹¨ì–´ ë²¡í„°ê°€ ê°€ì§€ëŠ” ëª‡ëª‡ ë¬¸ì œë“¤ì„ ê·¹ë³µí•˜ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ë²•ì€ ë¬¸ì ì»¨ë³¼ë£¨ì…˜ì„ í†µí•´ ë¶€ë¶„ ë¬¸ìì— ëŒ€í•œ ì´ì ì„ ê°€ì§€ê³ , ì‚¬ì „ ì •ì˜ëœ ì˜ë¯¸ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ëª…ì‹œì ì¸ í›ˆë ¨ ì—†ì´ ë‹¤ì˜ì–´ ì •ë³´ë¥¼ ì›í™œí•˜ê²Œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.

&emsp;ë˜í•œ ë‹¤ë¥¸ ìµœê·¼ì˜ ì—°êµ¬ëŠ” êµ¬ë¬¸ì— ì˜ì¡´ì ì¸ representationì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. context2vecì€ ëŒ€ìƒ ì£¼ìœ„ì˜ ë¬¸ë§¥ì„ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ ì–‘ë°©í–¥ LSTMì„ ì‚¬ìš©í•©ë‹ˆë‹¤. contextual embeddingì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ë“¤ì€ representationì— pivot word ìì²´ë¥¼ í¬í•¨í•˜ê³  ì§€ë„ í•™ìŠµ ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­(CoVe) ë˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ ì–¸ì–´ ëª¨ë¸ì˜ ì¸ì½”ë”ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. ê¸°ê³„ ë²ˆì—­ ë°©ì‹ì´ ë³‘ë ¬ ë§ë­‰ì¹˜ì˜ í¬ê¸°ì— ì œí•œì„ ë°›ì§€ë§Œ ì´ëŸ¬í•œ ë°©ì‹ë“¤ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ì´ì ì„ ê°€ì§‘ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ, ìš°ë¦¬ëŠ” í’ë¶€í•œ ë‹¨ì¼ ì–¸ì–´ ë°ì´í„°ì— ì ‘ê·¼í•¨ìœ¼ë¡œì¨ ì–»ëŠ” ìµœëŒ€í•œì˜ ì´ì ì„ ì–»ê³ , ì•½ 3ì²œë§Œ ê°œì˜ ë¬¸ì¥ì´ í¬í•¨ëœ ë§ë­‰ì¹˜ë¡œ biLMì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ë˜í•œ ìš°ë¦¬ëŠ” deep contextual representationì— ëŒ€í•œ ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ë²•ë“¤ì„ ë„“ì€ ë²”ìœ„ì˜ ë‹¤ì–‘í•œ NLP ê³¼ì œì— ëŒ€í•´ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì„ ë³´ì—¬ ì¼ë°˜í™” ì‹œì¼°ìŠµë‹ˆë‹¤.

&emsp;ì´ì „ì˜ ì—°êµ¬ë“¤ì€ ì–‘ë°©í–¥ RNNì˜ ë ˆì´ì–´ë“¤ì´ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, deep LSTMì˜ í•˜ìœ„ ë ˆì´ì–´ì— multi-task êµ¬ë¬¸ ê°ë…(e.g., í’ˆì‚¬ íƒœê¹…)ì„ ë„ì…í•˜ëŠ” ê²ƒì€ ì¢…ì†ì„± êµ¬ë¬¸ ë¶„ì„ì´ë‚˜ CCG super taggingê³¼ ê°™ì€ ë†’ì€ ë ˆë²¨ì˜ taskì—ì„œ ì „ì²´ì ì¸ ì„±ëŠ¥ì„ ê°œì„ ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. RNN ê¸°ë°˜ ì¸ì½”ë”-ë””ì½”ë” ê¸°ê³„ ë²ˆì—­ ì‹œìŠ¤í…œì—ì„œ, Belinkov et al. (2017)ì€ 2-layer LSTM ì¸ì½”ë”ì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ì—ì„œ í•™ìŠµëœ representationì´ ë‘ ë²ˆì§¸ ë ˆì´ì–´ì— ë¹„í•´ í’ˆì‚¬ íƒœê·¸ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì— ë” ë‚«ë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë‹¨ì–´ ë¬¸ë§¥ ì¸ì½”ë”©ì„ ìœ„í•œ LSTMì˜ ìµœìƒìœ„ ë ˆì´ì–´ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ì— ëŒ€í•œ representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¦ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ELMo representationsì˜ ìˆ˜ì •ëœ ì–¸ì–´ ëª¨ë¸ ëª©ì ì— ì˜í•´ ìœ ì‚¬í•œ ì‹ í˜¸ë“¤ì´ ìœ ë„ë¨ì„ ë³´ì˜€ê³ , ì´ëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¤€ê°ë…ì„ í˜¼í•©í•˜ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ í•œ ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ëŠ” ê²ƒì— ë§¤ìš° ìœ ìµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&emsp;Dai and Le (2015)ì™€ Ramachandran et al. (2017)ì€ ì¸ì½”ë”-ë””ì½”ë” ìŒì„ ì–¸ì–´ ëª¨ë¸ê³¼ sequence autoencoderë¥¼ ì‚¬ìš©í•´ ì‚¬ì „ì— í•™ìŠµì‹œì¼°ê³  taskë³„ ì§€ë„ë¥¼ í†µí•´ fine tuningí•˜ì˜€ë‹¤. ì´ì™€ ë°˜ëŒ€ë¡œ, ìš°ë¦¬ëŠ” biLMì„ ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë°ì´í„°ë¡œ í•™ìŠµì‹œí‚¨ ë’¤ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•˜ê³  taskì— ë”°ë¼ ëª¨ë¸ ìš©ëŸ‰ì„ ì¶”ê°€í•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í›ˆë ¨ ë°ì´í„°ê°€ ë” ì‘ì€ ì§€ë„ í•™ìŠµ ëª¨ë¸ì„ ì§€ì‹œí•˜ëŠ” ê²½ìš° í¬ê³  í’ë¶€í•˜ê³  ë³´í¸ì ì¸ biLM representationì„ í™œìš©í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
## 3. ELMo: Embeddings from Language Models
&emsp;ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ ì„ë² ë”©ë“¤ê³¼ ë‹¬ë¦¬, ELMo word representationì€ ì´ ì„¹ì…˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ì „ì²´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤ . ê·¸ë“¤ì€ ë¬¸ì ì»¨ë³¼ë£¨ì…˜ì„ í†µí•´ (Sec. 3.1), ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¡œì¨ 2-layer biLMì˜ ìµœìƒìœ„ layerì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤ (Sec. 3.2). ì´ ì„¤ì •ì„ í†µí•´ ìš°ë¦¬ëŠ” biLMì´ í° ê·œëª¨ì—ì„œ ì‚¬ì „ì— í•™ìŠµë˜ì–´ ìˆì„ ë•Œ (Sec. 3.4) ì¤€ì§€ë„ í•™ìŠµì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ë„“ì€ ë²”ìœ„ì˜ ê¸°ì¡´ì˜ neural NLP ì•„í‚¤í…ì²˜ì— í†µí•©ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Sec. 3.3).

### 3.1 Bidirectional langualge models
&emsp; Nê°œì˜ token $(t_1, t_2, \dots, t_N)$ì´ ì£¼ì–´ì§€ë©´, ì •ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì€ $(t_1, \dots, t_{k-1})$ì´ ì£¼ì–´ì¡Œì„ ë•Œ $t_k$ì˜ í™•ë¥ ì„ ëª¨ë¸ë§í•˜ì—¬ ì‹œí€€ìŠ¤ì˜ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
$$
p(t_1, t_2, \dots, t_N)=\prod_{k=1}^{N} p(t_{k} | t_{1}, t_{2}, \dots t_{k-1})
$$
ìµœê·¼ì˜ ìµœì‹  ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì€ í† í° ì„ë² ë”© ë˜ëŠ” ë¬¸ì CNNì— ëŒ€í•œ í†µí•´ ë¬¸ë§¥ì— ë¹„ì˜ì¡´ì ì¸ token representation $\mathrm{x}_{k}^{LM}$ì„ ê³„ì‚°í•˜ê³  ì •ë°©í–¥ LSTMì˜ $L$ê°œì˜ layerë¥¼ í†µí•´ ì „ë‹¬í•©ë‹ˆë‹¤. ê° ìœ„ì¹˜ $k$ì—ì„œ, ê°ê°ì˜ LSTM layerëŠ” $j=1,\dots,L$ì—ì„œ ë¬¸ë§¥ì— ì˜ì¡´ì ì¸ representation $\overrightarrow{h}_{k,j}^{LM}$ì„ ì¶œë ¥í•©ë‹ˆë‹¤. LSTMì˜ ìµœìƒìœ„ ë ˆì´ì–´ì˜ ì¶œë ¥ì€ Softmax layerë¥¼ í†µí•´ ë‹¤ìŒ token $t_{k+1}$ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

&emsp;ì—­ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì€ ì´í›„ì˜ ë¬¸ë§¥ì´ ì£¼ì–´ì§ˆ ë•Œ ì´ì „ì˜ tokenì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë¬¸ì¥ì„ ë°›ì•„ë“¤ì´ëŠ” ê²ƒì„ ì œì™¸í•˜ë©´ ì •ë°©í–¥ ì–¸ì–´ ëª¨ë¸ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.
$$
p(t_1, t_2, \dots, t_N)=\prod_{k=1}^{N} p(t_{k} | t_{k+1}, t_{k+2}, \dots t_{N})
$$
$(t_{k+1}, t_{k+2}, \dots t_{N})$ì´ ì£¼ì–´ì§ˆ ë•Œ $t_{k}$ì˜ representation $\overleftarrow{h}_{k,j}^{LM}$ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” $L$ layer backward LSTMì˜ ë ˆì´ì–´ $j$ì—ì„œ ì •ë°©í–¥ ì–¸ì–´ ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&emsp;biLMì€ ì •ë°©í–¥ê³¼ ì—­ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì„ ê²°í•©í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê³µì‹ì€ ì •ë°©í–¥ê³¼ ì—­ë°©í–¥ì˜ log likelihoodë¥¼ ëª¨ë‘ ìµœëŒ€í™”í•©ë‹ˆë‹¤.
$$
\sum_{k=1}^N(\log p(t_{k} | t_{1}, t_{2}, \dots t_{k-1};\Theta_{x}, \overrightarrow{\Theta}_{LSTM}, \Theta_{s}) + \\
\log p(t_{k} | t_{k+1}, t_{k+2}, \dots t_{N};\Theta_{x}, \overleftarrow{\Theta}_{LSTM}, \Theta_{s}))
$$
ìš°ë¦¬ëŠ” ê° ë°©í–¥ì˜ LSTMì´ ê°€ì§€ëŠ” parameterë¥¼ ìœ ì§€í•˜ë©´ì„œ token representation($\Theta_{x}$)ê³¼ Softmax layer($\Theta_{s}$)ì— í•„ìš”í•œ parameterë¥¼ ì •ë°©í–¥ê³¼ ì—­ë°©í–¥ì— ì—°ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì™„ì „íˆ ë…ë¦½ëœ parameterë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ì— ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•œ ë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì „ì²´ì ìœ¼ë¡œ ì´ ê³µì‹ì€ Peters et al. (2017)ì˜ ì ‘ê·¼ ë°©ë²•ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ì—ì„œ,  biLMì˜ ì„ í˜• ì¡°í•©ì¸ word representationì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì„ ë„ì…í•˜ë©° ì´ì „ì˜ ì—°êµ¬ë“¤ë¡œ ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
## 3.2 ELMo
&emsp;ELMoëŠ” taskì— íŠ¹ì´ì ì¸ biLMì˜ intermediate layer representationì˜ ì¡°í•©ì…ë‹ˆë‹¤. ê°ê°ì˜ token $t_{k}$ì— ëŒ€í•˜ì—¬, $L$-layer biLMì€ $2L+1$ê°œì˜ representationì„ ê³„ì‚°í•©ë‹ˆë‹¤.
$$
\begin{align}
R_{k} &= \{x_{k}^{LM}, \overrightarrow{h}_{k,j}^{LM}, \overleftarrow{h}_{k,j}^{LM} |j=1,\dots,L\}\\
&= \{h_{k,j}^{LM}|j=0, \dots, L\}
\end{align}
$$
ì´ë•Œ $h_{k,0}^{LM}$ì€ token layerì´ê³  ê° ì–‘ë°©í–¥ LSTMì—ì„œ $h_{k,j}^{LM}=\left[\overrightarrow{h}_{k,j}^{LM}, \overleftarrow{h}_{k,j}^{LM} \right]$ì…ë‹ˆë‹¤.

&emsp;ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì— ë„ì…í•˜ê¸° ìœ„í•´, ELMoëŠ” $R$ì˜ ëª¨ë“  ë ˆì´ì–´ë¥¼ ë‹¨ì¼ ë²¡í„°ë¡œ í†µí•©í•©ë‹ˆë‹¤ $\mathrm{ELMo}_{k}=E(R_{k};\Theta_{e})$. ê°€ì¥ ë‹¨ìˆœí•œ ê²½ìš°, ELMoëŠ” TagLM (Peters et al., 2017) ê·¸ë¦¬ê³  CoVe (Mc-Cann et al., 2017) ì™€ ê°™ì´ ìµœìƒìœ„ ë ˆì´ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ $E(R_{k})=h_{k,L}^{LM}$. ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” taskì— íŠ¹ì´ì ì¸ ëª¨ë“  biLM ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤.
$$
\mathrm{ELMo}_{k}^{task}=E(R_{k};\Theta^task)=\gamma^{task} \sum_{j=0}^{L} s_{j}^{task} h_{k,j}^{LM} 
\tag{1}
$$
(1)ì—ì„œ,  $s^{task}$ëŠ” Softmaxì— ì˜í•´ ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ì´ê³  scalar parameter $\gamma^{task}$ëŠ” task modelì´ ELMo vector ì „ì²´ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. $\gamma$ëŠ” ìµœì í™” ê³¼ì •ì—ì„œ ì‹¤ì§ˆì ìœ¼ë¡œ ì¤‘ìš”í•©ë‹ˆë‹¤ (ìì„¸í•œ ë‚´ìš©ì€ ë³´ì¶© ìë£Œ ì°¸ì¡°). ê°ê°ì˜ biLM ë ˆì´ì–´ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ê³ ë ¤í•  ë•Œ, ì¼ë¶€ ê²½ìš°ì—ì„œ ì´ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê¸° ì „ì— ê° biLM ë ˆì´ì–´ì— layer normalizationì„ ì¶”ê°€í•˜ëŠ” ë°ì— ë„ì›€ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤ (Ba et al., 2016).

## 3.3 Using biLMs for supervised NLP tasks
&emsp;ì‚¬ì „ í•™ìŠµëœ biLMê³¼ target NLP taskë¥¼ ìœ„í•œ ì§€ë„ í•™ìŠµ ì•„í‚¤í…ì²˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, task modelì„ ê°œì„ í•˜ê¸° ìœ„í•´ biLMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ biLMì„ ì‘ë™ì‹œí‚¤ê³ , ê° ë‹¨ì–´ì— ëŒ€í•œ ëª¨ë“  ë ˆì´ì–´ì˜ representationì„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ, ìš°ë¦¬ëŠ” í›„ìˆ  ë˜ì–´ ìˆë“¯ì´ ë§ˆì§€ë§‰ task modelì´ ì´ëŸ¬í•œ representationì˜ ì„ í˜• ì¡°í•©ì„ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

&emsp;ìš°ì„  biLM ì—†ì´ ì§€ë„ í•™ìŠµëœ ëª¨ë¸ì˜ ìµœí•˜ìœ„ ë ˆì´ì–´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì§€ë„ í•™ìŠµëœ NLP ëª¨ë¸ë“¤ì€ ìµœí•˜ìœ„ ë ˆì´ì–´ì—ì„œ ê³µí†µëœ ì•„í‚¤í…ì²˜ë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— ì¼ê´€ì ì´ê³  í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ELMoë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tokenìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì„œì—´ $(t_{1}, \dots, t_{N})$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê° tokenì˜ ìœ„ì¹˜ì— ëŒ€í•´ì„œ ì‚¬ì „ í•™ìŠµëœ ë‹¨ì–´ ì„ë² ë”©ê³¼ ì„ íƒì ìœ¼ë¡œ ë¬¸ì ê¸°ë°˜ì˜ representationì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ ì˜ì¡´ì ì¸ token representation $\mathrm{x}_{k}$ë¥¼ í˜•ì„±í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ modelì´ ë¬¸ë§¥ì— ë¯¼ê°í•œ represeentation $h_{k}$ë¥¼ í˜•ì„±í•˜ë©°, ì „í˜•ì ìœ¼ë¡œ bidirectional RNN, CNN ë˜ëŠ” feed forward networkê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
&emsp;ELMoë¥¼ ì§€ë„ í•™ìŠµëœ ëª¨ë¸ì— ì¶”ê°€í•˜ê¸° ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” ìš°ì„  biLMì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³  ELMo vector $\mathrm{ELMo}_{k}^{task}$ì™€ $\mathrm{x}_{k}$ë¥¼ concatenateí•˜ì—¬ ELMo enhanced representation $\left[\mathrm{x}_{k};\mathrm{ELMo}_{k}^{task}\right]$ë¥¼ task RNNìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì¼ë¶€ task(e.g., SNLI, SQuAD)ì˜ ê²½ìš° ìƒˆë¡œìš´ ì¶œë ¥ ê°€ì¤‘ì¹˜ë¥¼ ë„ì…í•˜ê³  $h_{k}$ë¥¼ $\left[h_{k};\mathrm{ELMo}_{k}^{task}\right]$ë¡œ ëŒ€ì²´í•˜ì—¬ task RNNì˜ ì¶œë ¥ì— ELMoë¥¼ ë„ì…í•¨ìœ¼ë¡œì¨ ì¶”ê°€ì ì¸ ê°œì„ ì„ ê´€ì¸¡í•˜ì˜€ìŠµë‹ˆë‹¤. ì§€ë„ í•™ìŠµëœ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë³€ê²½ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ ë„ì…ì€ ë³µì¡í•œ ì‹ ê²½ë§ ëª¨ë¸ì˜ ë§¥ë½ì—ì„œ ì¼ì–´ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <font color="#ff0000">ì˜ˆë¥¼ ë“¤ì–´, bi-attention layerê°€ biLSTMì„ ë’¤ë”°ë¥¼ ë•Œì¸ Sec. 4ì˜ SNLI ì‹¤í—˜ ë˜ëŠ” biLSTMì˜ ìµœìƒìœ„ ë ˆì´ì–´ì— clustering modelì´ ì¶”ê°€ ë˜ì—ˆì„ ë•Œ ìƒí˜¸ ì°¸ì¡° í•´ê²° ì‹¤í—˜ë“¤ì„ ë³´ì.</font>

&emsp;ìµœì¢…ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ELMoì— ì ë‹¹í•œ ì–‘ì˜ ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•˜ëŠ” ê²ƒê³¼ (Srivastava et al., 2014) ëª‡ëª‡ ìƒí™©ì—ì„œ ELMoì˜ ê°€ì¤‘ì¹˜ì— $\lambda ||\mathrm{w}||_{2}^2$ ì„ ë”í•˜ì—¬ ê·œì œí•˜ëŠ” ê²ƒì´ ìœ ìµí•˜ë‹¤ëŠ” ê²ƒì„ ì•Œì•„ë‚´ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë“  biLM ë ˆì´ì–´ì˜ í‰ê· ì— ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ ELMoì˜ ê°€ì¤‘ì¹˜ì—  biasì˜ ìœ ë„ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.
## 3.4 Pre-trained bidirectional language model architecture
&emsp;ì´ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ì‚¬ì „ í•™ìŠµëœ biLMì€ Jozefowicz et al. (2016) ë° Kim et al. (2015)ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ì–‘ë°©í–¥ì˜ í•©ë™ í›ˆë ¨ì„ ìœ„í•´ ìˆ˜ì •ë˜ì—ˆê³  LSTM ë ˆì´ì–´ ì‚¬ì´ì— residual connectionì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ì—°êµ¬ì—ì„œ Peters et al. (2017)ì´ ì •ë°©í–¥ LMê³¼ ëŒ€ê·œëª¨ í•™ìŠµì—ì„œ biLMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ì˜€ë“¯ì´, ëŒ€ê·œëª¨ biLMì— ì§‘ì¤‘í•˜ì˜€ìŠµë‹ˆë‹¤.

&emsp;ë¬¸ì ê¸°ë°˜ ì…ë ¥ representationì„ ìœ ì§€í•  ë•Œ, ì „ì²´ì ì¸ ëª¨ë¸ì˜ ë³µì¡ì„±ê³¼ ëª¨ë¸ì˜ í¬ê¸°, ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ taskë¥¼ ìœ„í•´ ìš”êµ¬ë˜ëŠ” ê³„ì‚°ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” Jozefowicz et al. (2016)ì˜ CNN-BIG-LSTMì—ì„œ ëª¨ë“  ì„ë² ë”© ë°  íˆë“  ë ˆì´ì–´ì˜ ì°¨ì›ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.<font color="#ff0000"> ìµœì¢… ëª¨ë¸ì€ 4096ì˜ unitê³¼ 512ê°œì˜ ì°¨ì› íˆ¬ì˜ ê·¸ë¦¬ê³  ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ë ˆì´ì–´ ì‚¬ì´ì— residual connectionì„ ê°€ì§„ $L=2$ biLSTMì…ë‹ˆë‹¤.</font> context insensitive type representationì€ 2048 ë¬¸ì n-ê·¸ë¨ ì»¨ë³¼ë£¨ì…˜ í•„í„°ì™€ ë‘ ê°œì˜ highway layer (Srivastava et al., 2015) ê·¸ë¦¬ê³  512 representationìœ¼ë¡œì˜ ì„ í˜• ì‚¬ì˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ë¡œ, biLM




## References
[1] Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR abs/1607.06450. 
[2] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. 2017. What do neural machine translation models learn about morphology? In ACL. 
[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL 5:135â€“146. 
[4] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. 
[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH. 
[6] Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In ACL. 
[7] Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. In TACL. 
[8] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP. 
[9] Christopher Clark and Matthew Gardner. 2017. Simple and effective multi-paragraph reading comprehension. CoRR abs/1710.10723. 
[10] Kevin Clark and Christopher D. Manning. 2016. Deep reinforcement learning for mention-ranking coreference models. In EMNLP. 
[11] Ronan Collobert, Jason Weston, Leon Bottou, Michael Â´ Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. In JMLR. 
[12] Andrew M. Dai and Quoc V. Le. 2015. Semisupervised sequence learning. In NIPS. 
[13]Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In EMNLP. 
[14] Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In NIPS. 
[15] Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natural language inference over interaction space. In ICLR. 
[16] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In EMNLP 2017. 
[17] Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and whatâ€™s next. In ACL. 
[18] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long Â¨ short-term memory. Neural Computation 9. 
[19] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In ACL. 
[20] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Â´ Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. CoRR abs/1602.02410. 
[21] Rafal Jozefowicz, Wojciech Zaremba, and Ilya Â´ Sutskever. 2015. An empirical exploration of recurrent network architectures. In ICML. 
[22] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2015. Character-aware neural language models. In AAAI 2016. 
[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR. 
[24] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML.
[25] John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML. 
[26] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT. 
[27] Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In EMNLP. 
[28] Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, LuÂ´Ä±s Marujo, and Tiago LuÂ´Ä±s. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In EMNLP. 
[29] Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556 . 
[30] Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In ACL. 
[31] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics 19:313â€“330. 
[32] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017. 
[33] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL. 
[34] Gabor Melis, Chris Dyer, and Phil Blunsom. 2017. On Â´ the state of the art of evaluation in neural language models. CoRR abs/1707.05589. 
[35] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. CoRR abs/1708.02182. 
[36] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. 
[37] George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In HLT. 
[38 ]Tsendsuren Munkhdalai and Hong Yu. 2017. Neural tree indexers for text understanding. In EACL. 
[39] Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In EMNLP. 
[40] Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31:71â€“ 106. 
[41] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP. 
[42] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. 
[43] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bjorkelund, Olga Uryupina, Â¨ Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL. 
[44] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In EMNLPCoNLL Shared Task. 
[45] Alessandro Raganato, Claudio Delli Bovi, and Roberto Navigli. 2017a. Neural sequence learning models for word sense disambiguation. In EMNLP. 
[46] Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017b. Word sense disambiguation: A unified evaluation framework and empirical comparison. In EACL. 
[47] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP. 
[48] Prajit Ramachandran, Peter Liu, and Quoc Le. 2017. Improving sequence to sequence learning with unlabeled data. In EMNLP. 
[49] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL. 
[50] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR. 
[51] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP. 
[52] Anders SÃ¸gaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In ACL 2016. 
[53] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15:1929â€“1958. 
[54] Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Â¨ Schmidhuber. 2015. Training very deep networks. In NIPS. 
[55] Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL. 
[56] Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In ACL. 
[57] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP. 
[58] Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coreference resolution. In HLT-NAACL. 
[59] Matthew D. Zeiler. 2012. Adadelta: An adaptive learning rate method. CoRR abs/1212.5701. 
[60] Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In ACL. 
[61] Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classification improved by integrating bidirectional lstm with twodimensional max pooling. In COLING