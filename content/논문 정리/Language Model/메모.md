### embedding
임베딩은 텍스트, 이미지 등 자료에 국한되지 않는다. 정보 그 자체를 나타내는 것.

*문법적으로 유사한 문장들이 많이 포함된 데이터를 dedup하는 연구는 없나?*
- 문장의 구조적 유사도를 어떻게 표현할까?
  1. 형태소 분석기로 pos tagging까지 하고 각 단어에 대해 one-hot expression으로 표현?

*dedup이 유효한 이유는 문장이 주어졌을 때 모델이 문법적, 의미론적 등의 관점을 개별적으로 다루기 때문?*

*clustering 했을 때 해당 그룹을 대표하는 몇 개의 데이터 찾기?*

kernel trick으로 embedded vector를 더 잘 분류하기 위한 모수 추정?


### deduplication
기존의 방식들은 스케일링 법칙의 멱법칙 특성을 이용하여 더 많은 데이터를 통해 성능을 증가시킴. 하지만 중복되는 데이터에 대한 학습은 학습량의 낭비이다. 중복되는 데이터를 제거하여 고품질의 데이터를 만들자.







또 다른 작업의 한 흐름은 언어 분야별 가중치 최적화입니다. 예를 들어 훈련용 말뭉치가 있다고 했을 때, 이 문장들의 임베딩을 계산하고 클러스터링을 하고 난 후의, 각각의 클러스터를 데이터 도메인이라고 부른다고 해봅시다. DoReMi 및 그 후속 모델인 DoGE과 같은 방법은 작은 중간 모델 (proxy model) 을 사용하여 주어진 도메인을 학습하는 데 더 큰 모델이 얼마나 많은 토큰을 필요로 하는지를 추정할 수 있습니다. 이렇게 하면 정보량이 별로 없거나(너무 쉽거나) 노이즈가 많은(너무 어려운) 도메인에 귀중한 컴퓨팅 및 모델 용량을 낭비하는 것을 방지할 수 있습니다.
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining
https://arxiv.org/abs/2305.10429
DoGE: Domain Reweighting with Generalization Estimation
https://arxiv.org/abs/2310.15393





개인이 pretrain 할 수 있을까? - instructkr/ko-wand-136M
https://arca.live/b/alpaca/95522991?target=all&keyword=%ED%92%88%EC%A7%88&p=1

AI로 좋은 퀄리티의 데이터셋을 감별하자
https://arca.live/b/alpaca/88533334?target=all&keyword=%ED%92%88%EC%A7%88&p=2