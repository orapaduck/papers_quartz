### 1 Abstract
최근 몇 년 동안 대규모 웹 말뭉치에서 무작위로 선택된 가능한 한 많은 토큰에 대해 원패스 학습을 수행하여 대규모 언어 모델(LLM)을 훈련하는 데 점점 더 많은 양의 컴퓨팅 및 데이터가 쏟아졌습니다. 인터넷의 훨씬 더 큰 부분에 대한 훈련은 일관된 성능 향상으로 이어지지만, 이러한 개선의 크기는 규모에 따라 감소하며, MinHash와 같은 단순한 중복 제거 방법을 넘어 데이터 선택이 사전 훈련 및 다운스트림 성능에 미치는 영향을 탐구하는 작업은 거의 없었습니다. 여기서는 사전 학습된 모델 임베딩을 통해 신중한 데이터 선택(deduplicated data에서도)을 통해 학습 속도를 높이고(효율성 향상 20%) 6.7B 모델 규모에서 16개의 NLP 작업(최대 2%)에 대한 평균 다운스트림 정확도를 향상시킬 수 있음을 보여줍니다. 또한 데이터를 반복하면 일관되게 기준선 훈련보다 성능이 우수한 반면 무작위 데이터를 반복하면 기준선 훈련보다 성능이 떨어지는 것으로 나타났습니다. 우리의 결과는 영리한 데이터 선택이 LLM 사전 훈련을 크게 개선할 수 있고, 가능한 한 많은 데이터에 대해 단일 시대에 대한 훈련의 일반적인 관행에 의문을 제기하며, 웹 데이터를 무작위로 샘플링하는 한계를 넘어 모델을 계속 개선할 수 있는 경로를 보여줍니다.
### 2 정리
- SSL prototype의 성능이 중복 기반 cluster의 영향을 받음. SemDedup을 활용하여 이를 해결한 D4 제시
- 고정 토큰 한도로 무한한 데이터와 훈련 모델이 있는 체제에서 무작위 iid 데이터 선택 및 이전에 사용된 방법보다 더 높은 perplexity 및 downstream acc를 얻을 수 있음을 보임. 모델 규모에 따라 효율성이 향상의 크기가 증가한다.
- epoch마다 랜덤한 데이터를 선택하여 학습시키는 것은 오히려 데이터의 추가보다 성능이 떨어지며, 계산을 통해 선택된 데이터를 통한 학습이 랜덤한 것보다 나음.
#### D4
MinHash로 제거되지 않은 templated text나 의미론적으로 중복된 데이터를 찾는다. 이러한 임베딩 공간의 영역들은 매우 밀집되어있어, 동일한 그룹에 클러스터를 낭비하게된다.
1. 전체 데이터셋 $D$에서 selection ratio $R_{dedup}$에 따라 SemDedup을 적용하고, 이를 $D'$이라고 한다.
2. K-Means를 통해 $D'$의 cluster point를 얻는다.
3. $D'$에서 $R_{proto}$에 따라 SSL Prototype을 수행한다.

Tirumala, Kushal, et al. "D4: Improving llm pretraining via document de-duplication and diversification." _Advances in Neural Information Processing Systems_ 36 (2024).